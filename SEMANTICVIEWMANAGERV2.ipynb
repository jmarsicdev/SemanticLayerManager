{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "yjkoounjgnsicwn5brl7",
   "authorId": "9050488606829",
   "authorName": "JMARSICDEV",
   "authorEmail": "marsicojo@gmail.com",
   "sessionId": "5f6fe482-5eea-45a4-98e1-c178dd914ab0",
   "lastEditTime": 1765385812741
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "source": "import streamlit as st\nfrom snowflake.snowpark.context import get_active_session\nfrom snowflake.snowpark.exceptions import SnowparkSQLException\nimport json\nimport yaml\n\n# Get the active session\nsession = get_active_session()\n\ndef fetch_columns(database: str, schema: str, table: str) -> list:\n    query = f\"\"\"\n        SELECT COLUMN_NAME, DATA_TYPE\n        FROM {database}.INFORMATION_SCHEMA.COLUMNS\n        WHERE TABLE_SCHEMA = '{schema.upper()}' AND TABLE_NAME = '{table.upper()}'\n        ORDER BY ORDINAL_POSITION;\n    \"\"\"\n    try:\n        columns = session.sql(query).collect()\n        return [row.as_dict() for row in columns]\n    except SnowparkSQLException as e:\n        st.error(f\"Error fetching columns: {e.message}\")\n        return []\n\ndef generate_ai_description(column_name: str, data_type: str) -> dict:\n    prompt = f\"\"\"\n        You are a data analyst writing a semantic model for a table.\n        Given a column named '{column_name}' with data type '{data_type}':\n        1. Write a concise, one-sentence, business-friendly description.\n        2. Suggest 2-3 common business synonyms.\n        Return ONLY a JSON object with keys \"description\" and \"synonyms\".\n    \"\"\"\n    try:\n        # Notebooks can call Cortex SQL directly\n        cortex_query = f\"SELECT SNOWFLAKE.CORTEX.COMPLETE('mistral-large', {prompt!r})\"\n        response_str = session.sql(cortex_query).collect()[0][0]\n        # JSON extraction in case model adds text around it\n        json_str = response_str[response_str.find('{'):response_str.rfind('}')+1]\n        return json.loads(json_str)\n    except Exception:\n        return {\"description\": f\"Data for {column_name}\", \"synonyms\": [column_name]}\n\n# Generation - builds YAML string - reuses definitions if possible\ndef generate_semantic_model(database: str, schema: str, table: str, known_defs: dict = None) -> str:\n    \n    # Initialize empty dict if None passed\n    if known_defs is None:\n        known_defs = {}\n\n    columns = fetch_columns(database, schema, table)\n    if not columns: return \"\"\n\n    dimensions = []\n    measures = []\n    \n    st.write(f\"Processing {len(columns)} columns...\")\n    \n    # Progress bar for better UI experience\n    my_bar = st.progress(0)\n\n    for i, col in enumerate(columns):\n        col_name = col[\"COLUMN_NAME\"]\n        col_type = col[\"DATA_TYPE\"]\n        \n        # Check central table first\n        if col_name in known_defs:\n            # HIT: Use the saved definition\n            ai_meta = known_defs[col_name]\n        else:\n            # MISS: Ask Cortex\n            ai_meta = generate_ai_description(col_name, col_type)\n        \n        # Map snowflake type to semantic type\n        semantic_type = col_type\n        if any(x in col_type for x in ['VARCHAR', 'TEXT', 'CHAR']): semantic_type = 'TEXT'\n        elif any(x in col_type for x in ['NUMBER', 'INT', 'FLOAT']): semantic_type = 'NUMBER'\n        elif 'DATE' in col_type: semantic_type = 'DATE'\n        elif 'TIMESTAMP' in col_type: semantic_type = 'TIMESTAMP'\n        elif 'BOOLEAN' in col_type: semantic_type = 'BOOLEAN'\n\n        entry = {\n            'name': col_name,\n            'expr': col_name,\n            'data_type': semantic_type,\n            'description': ai_meta.get('description'),\n            'synonyms': ai_meta.get('synonyms', [])\n        }\n        \n        # Classify dimension vs. measure\n        if col_type in ('NUMBER', 'FLOAT', 'DECIMAL', 'INT', 'DOUBLE'):\n            measures.append(entry)\n        else:\n            dimensions.append(entry)\n            \n        # Update progress\n        my_bar.progress((i + 1) / len(columns))\n\n    # Build structure\n    model_dict = {\n        \"name\": table,\n        \"description\": f\"Semantic model for {table}\",\n        \"tables\": [{\n            \"name\": table,\n            \"base_table\": {\n                \"database\": database,\n                \"schema\": schema,\n                \"table\": table\n            },\n            \"dimensions\": dimensions,\n            \"measures\": measures\n        }]\n    }\n    \n    return yaml.dump(model_dict, sort_keys=False, default_flow_style=False)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "f7745128-bb37-41e6-96d4-7301dd30ef75",
   "metadata": {
    "name": "cell4",
    "collapsed": false
   },
   "source": "CENTRAL SEMANTIC DATABASE"
  },
  {
   "cell_type": "code",
   "id": "34515587-651b-4c4a-bb69-164393d7fa8b",
   "metadata": {
    "language": "python",
    "name": "cell3"
   },
   "outputs": [],
   "source": "import json\nimport yaml\n\n# Fetch existing definitions\ndef get_known_attributes(column_names):\n    if not column_names:\n        return {}\n    \n    # Format list for SQL IN clause\n    formatted_cols = \", \".join([f\"'{c.upper()}'\" for c in column_names])\n    \n    try:\n        # Query the repository\n        df = session.sql(f\"\"\"\n            SELECT ATTRIBUTE_NAME, DESCRIPTION, SYNONYMS \n            FROM CORTEX_ANALYST_DEMO.REVENUE_TIMESERIES.SEMANTIC_ATTRIBUTE_REPOSITORY \n            WHERE ATTRIBUTE_NAME IN ({formatted_cols})\n        \"\"\").collect()\n        \n        # Convert to dictionary for easy lookup\n        known_defs = {}\n        for row in df:\n            known_defs[row['ATTRIBUTE_NAME']] = {\n                \"description\": row['DESCRIPTION'],\n                \"synonyms\": json.loads(row['SYNONYMS']) if row['SYNONYMS'] else []\n            }\n        return known_defs\n    except Exception as e:\n        return {}\n\n# Save/Upsert new definitions\ndef save_attributes_to_repo(yaml_content):\n    try:\n        data = yaml.safe_load(yaml_content)\n        fields_to_upsert = []\n        \n        # Extract columns/measures from standard Cortex YAML structure\n        if 'tables' in data:\n            for table in data['tables']:\n                # Process columns/dimensions\n                if 'columns' in table: \n                    for col in table['columns']:\n                        name = col.get('name')\n                        desc = col.get('description', '')\n                        synonyms = col.get('synonyms', [])\n                        # Only save if we have a valid description\n                        if name and desc:\n                            fields_to_upsert.append((name, desc, synonyms))\n                            \n                # Process measures\n                if 'measures' in table:\n                    for meas in table['measures']:\n                        name = meas.get('name')\n                        desc = meas.get('description', '')\n                        synonyms = meas.get('synonyms', [])\n                        if name and desc:\n                            fields_to_upsert.append((name, desc, synonyms))\n\n        # Perform Merge (Upsert) into snowflake table\n        if fields_to_upsert:\n            # Create a temporary dataframe\n            input_df = session.create_dataframe(fields_to_upsert, schema=[\"NAME\", \"DESC\", \"SYN\"])\n            input_df.create_or_replace_temp_view(\"TEMP_NEW_DEFS\")\n            \n            # Merge logic\n            session.sql(\"\"\"\n                MERGE INTO CORTEX_ANALYST_DEMO.REVENUE_TIMESERIES.SEMANTIC_ATTRIBUTE_REPOSITORY AS target\n                USING TEMP_NEW_DEFS AS source\n                ON target.ATTRIBUTE_NAME = source.NAME\n                WHEN MATCHED THEN UPDATE SET\n                    target.DESCRIPTION = source.DESC,\n                    target.SYNONYMS = source.SYN,\n                    target.LAST_UPDATED = CURRENT_TIMESTAMP()\n                WHEN NOT MATCHED THEN INSERT\n                    (ATTRIBUTE_NAME, DESCRIPTION, SYNONYMS, LAST_UPDATED)\n                    VALUES (source.NAME, source.DESC, source.SYN, CURRENT_TIMESTAMP())\n            \"\"\").collect()\n            \n            return len(fields_to_upsert)\n    except Exception as e:\n        print(f\"Repo update failed: {e}\")\n        return 0",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "848b99c4-e779-444a-875b-0d9f51bf2a78",
   "metadata": {
    "name": "cell5",
    "collapsed": false
   },
   "source": "SAVE FUNCTION AND UI"
  },
  {
   "cell_type": "code",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "python",
    "name": "cell2"
   },
   "source": "import streamlit as st\nimport yaml\n\n# Initialize Session State\nif \"yaml_content\" not in st.session_state:\n    st.session_state.yaml_content = \"\"\n\n# Notebook UI\nst.title(\"Semantic Layer Manager\")\n\n# Select Data\ncol1, col2 = st.columns([1, 1.5])\n\n# Selection & Generation\nwith col1:\n    try:\n        # Database Selection\n        db_list = [r[\"name\"] for r in session.sql(\"SHOW DATABASES\").collect()]\n        default_db = \"CORTEX_ANALYST_DEMO\" if \"CORTEX_ANALYST_DEMO\" in db_list else db_list[0]\n        selected_db = st.selectbox(\"Database\", db_list, index=db_list.index(default_db) if default_db in db_list else 0)\n        \n        # Schema Selection\n        schema_list = [r[\"name\"] for r in session.sql(f\"SHOW SCHEMAS IN DATABASE {selected_db}\").collect()]\n        default_schema = \"REVENUE_TIMESERIES\" if \"REVENUE_TIMESERIES\" in schema_list else schema_list[0]\n        selected_schema = st.selectbox(\"Schema\", schema_list, index=schema_list.index(default_schema) if default_schema in schema_list else 0)\n        \n        # Table Selection\n        table_list = [r[\"name\"] for r in session.sql(f\"SHOW TABLES IN SCHEMA {selected_db}.{selected_schema}\").collect()]\n        selected_table = st.selectbox(\"Table\", table_list)\n        \n    except Exception as e:\n        st.error(f\"Error loading metadata: {e}\")\n\n    # Generate Button\n    if st.button(\"Generate Model\"):\n        with st.spinner(\"Checking repository and generating...\"):\n            try:\n                # Get current table columns to check against repo\n                desc_df = session.sql(f\"DESC TABLE {selected_db}.{selected_schema}.{selected_table}\").collect()\n                current_columns = [r[\"name\"] for r in desc_df]\n\n                # Check Repo for existing definitions\n                known_defs = get_known_attributes(current_columns)\n                \n                if known_defs:\n                    st.info(f\"Found {len(known_defs)} existing definitions. Reusing them.\")\n\n                # Generate Model\n                st.session_state.yaml_content = generate_semantic_model(\n                    selected_db, \n                    selected_schema, \n                    selected_table,\n                    known_defs=known_defs\n                )\n            except Exception as e:\n                st.error(f\"Generation failed: {e}\")\n\n# Review & Save\nwith col2:\n    st.subheader(\"2. Review & Save\")\n    \n    # Editor\n    edited_yaml = st.text_area(\"YAML Preview\", value=st.session_state.yaml_content, height=500)\n    \n    st.markdown(\"---\")\n    \n    # Save Options\n    st.write(\"### Save as Semantic View\")\n    \n    # View Name\n    view_name = st.text_input(\"Semantic View Name\", value=f\"{selected_table}_SEMANTIC_VIEW\")\n\n    if st.button(\"Create Semantic View\"):\n        if not edited_yaml:\n            st.warning(\"Generate a model first!\")\n        else:\n            try:\n                # Define the target Schema (DB.SCHEMA only)\n                target_schema = f\"{selected_db}.{selected_schema}\"\n                \n                # Sync the \"View Name\" input with the YAML content\n                yaml_obj = yaml.safe_load(edited_yaml)\n                yaml_obj['name'] = view_name\n                final_yaml = yaml.dump(yaml_obj, sort_keys=False)\n\n                st.info(f\"Creating Semantic View '{view_name}' in schema '{target_schema}'...\")\n                \n                # Call the procedure\n                session.call(\n                    \"SYSTEM$CREATE_SEMANTIC_VIEW_FROM_YAML\",\n                    target_schema,\n                    final_yaml\n                )\n                \n                # Save definitions back to repository\n                count = save_attributes_to_repo(final_yaml)\n                \n                st.success(f\"Semantic View Created: {target_schema}.{view_name}\")\n                if count > 0:\n                    st.success(f\"Repository updated with {count} field definitions.\")\n                st.write(\"You can now find this view in Cortex Analyst.\")\n                    \n            except Exception as e:\n                st.error(f\"Failed to create view: {e}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "784d7ee1-b457-4368-b975-ab94d7ed9cce",
   "metadata": {
    "name": "cell7",
    "collapsed": false
   },
   "source": "Display the updates / current central database"
  },
  {
   "cell_type": "code",
   "id": "0961a37c-bb52-4ab6-90fa-d867c6cdc7ac",
   "metadata": {
    "language": "python",
    "name": "cell6"
   },
   "outputs": [],
   "source": "# Show the most recently updated attributes first\nsession.sql(\"\"\"\n    SELECT ATTRIBUTE_NAME, DESCRIPTION, LAST_UPDATED \n    FROM CORTEX_ANALYST_DEMO.REVENUE_TIMESERIES.SEMANTIC_ATTRIBUTE_REPOSITORY\n    ORDER BY LAST_UPDATED DESC\n    LIMIT 20\n\"\"\").show()",
   "execution_count": null
  }
 ]
}